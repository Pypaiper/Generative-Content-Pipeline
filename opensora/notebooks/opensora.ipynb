{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b916cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensora.acceleration.parallel_states import get_data_parallel_group\n",
    "from opensora.datasets.dataloader import prepare_dataloader\n",
    "from opensora.registry import DATASETS, build_module\n",
    "from opensora.utils.cai import (\n",
    "    get_booster,\n",
    "    get_is_saving_process,\n",
    "    init_inference_environment,\n",
    ")\n",
    "from opensora.utils.config import parse_alias, parse_configs\n",
    "from opensora.utils.inference import (\n",
    "    add_fps_info_to_text,\n",
    "    add_motion_score_to_text,\n",
    "    create_tmp_csv,\n",
    "    modify_option_to_t2i,\n",
    "    process_and_save,\n",
    ")\n",
    "from opensora.utils.logger import create_logger, is_main_process\n",
    "from opensora.utils.misc import log_cuda_max_memory, to_torch_dtype\n",
    "from opensora.utils.prompt_refine import refine_prompts\n",
    "from opensora.utils.sampling import (\n",
    "    SamplingOption,\n",
    "    prepare_api,\n",
    "    prepare_models,\n",
    "    sanitize_sampling_option,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import snapshot_download\n",
    "from colossalai.utils import set_seed\n",
    "import torch.distributed as dist\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from pprint import pformat\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe66b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148b99b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. configs & runtime variables\n",
    "# ======================================================\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# == parse configs ==\n",
    "cfg = parse_configs(\"../configs/diffusion/inference/t2i2v_256px.py\")\n",
    "cfg = parse_alias(cfg)\n",
    "cfg[\"prompt\"] = \"raining, sea\"\n",
    "cfg[\"ref\"] = \"../assets/texts/i2v.png\"\n",
    "\n",
    "model_path = Path(\"models\")\n",
    "if not model_path.exists():\n",
    "    model_path.mkdir()\n",
    "    REPO_ID = \"hpcai-tech/Open-Sora-v2\"\n",
    "    snapshot_download(repo_id=REPO_ID, local_dir=model_path)\n",
    "\n",
    "\n",
    "# == device and dtype ==\n",
    "\n",
    "device = \"cpu\"  # if not torch.cuda.is_available() else \"cuda\"\n",
    "dtype = to_torch_dtype(cfg.get(\"dtype\", \"bf16\"))\n",
    "seed = cfg.get(\"seed\", 1024)\n",
    "if seed is not None:\n",
    "    set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69c2f79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/29/25 17:17:09] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> colossalai - colossalai - INFO:                                                       \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/opt/conda/envs/opensora/lib/python3.10/site-packages/colossalai/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">initialize.py</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">75</span>     \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         launch                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/29/25 17:17:09]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m colossalai - colossalai - INFO:                                                       \n",
       "\u001b[2;36m                    \u001b[0m         \u001b[35m/opt/conda/envs/opensora/lib/python3.10/site-packages/colossalai/\u001b[0m\u001b[95minitialize.py\u001b[0m:\u001b[1;36m75\u001b[0m     \n",
       "\u001b[2;36m                    \u001b[0m         launch                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> colossalai - colossalai - INFO: Distributed environment is initialized, world size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m colossalai - colossalai - INFO: Distributed environment is initialized, world size: \u001b[1;36m1\u001b[0m \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# == init distributed env ==\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "init_inference_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d32de4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-07-29 17:17:11\u001b[0m] Inference configuration:\n",
      " {'ae': {'from_pretrained': 'models/hunyuan_vae.safetensors',\n",
      "        'in_channels': 3,\n",
      "        'latent_channels': 16,\n",
      "        'layers_per_block': 2,\n",
      "        'out_channels': 3,\n",
      "        'type': 'hunyuan_vae',\n",
      "        'use_spatial_tiling': True,\n",
      "        'use_temporal_tiling': False},\n",
      " 'batch_size': 1,\n",
      " 'clip': {'from_pretrained': 'models/openai/clip-vit-large-patch14',\n",
      "          'max_length': 77,\n",
      "          'type': 'text_embedder'},\n",
      " 'cond_type': 't2v',\n",
      " 'config_path': '../configs/diffusion/inference/t2i2v_256px.py',\n",
      " 'dataset': {'type': 'text'},\n",
      " 'dtype': 'bf16',\n",
      " 'fps_save': 24,\n",
      " 'img_flux': {'axes_dim': [16, 56, 56],\n",
      "              'cond_embed': False,\n",
      "              'context_in_dim': 4096,\n",
      "              'depth': 19,\n",
      "              'depth_single_blocks': 38,\n",
      "              'from_pretrained': './models/flux1-dev.safetensors',\n",
      "              'guidance_embed': True,\n",
      "              'hidden_size': 3072,\n",
      "              'in_channels': 64,\n",
      "              'mlp_ratio': 4.0,\n",
      "              'num_heads': 24,\n",
      "              'qkv_bias': True,\n",
      "              'theta': 10000,\n",
      "              'type': 'flux',\n",
      "              'vec_in_dim': 768},\n",
      " 'img_flux_ae': {'ch': 128,\n",
      "                 'ch_mult': [1, 2, 4, 4],\n",
      "                 'from_pretrained': 'models/flux1-dev-ae.safetensors',\n",
      "                 'in_channels': 3,\n",
      "                 'num_res_blocks': 2,\n",
      "                 'out_ch': 3,\n",
      "                 'resolution': 256,\n",
      "                 'scale_factor': 0.3611,\n",
      "                 'shift_factor': 0.1159,\n",
      "                 'type': 'autoencoder_2d',\n",
      "                 'z_channels': 16},\n",
      " 'img_resolution': '768px',\n",
      " 'model': {'axes_dim': [16, 56, 56],\n",
      "           'cond_embed': True,\n",
      "           'context_in_dim': 4096,\n",
      "           'depth': 19,\n",
      "           'depth_single_blocks': 38,\n",
      "           'from_pretrained': 'models/Open_Sora_v2.safetensors',\n",
      "           'fused_qkv': False,\n",
      "           'guidance_embed': False,\n",
      "           'hidden_size': 3072,\n",
      "           'in_channels': 64,\n",
      "           'mlp_ratio': 4.0,\n",
      "           'num_heads': 24,\n",
      "           'qkv_bias': True,\n",
      "           'theta': 10000,\n",
      "           'type': 'flux',\n",
      "           'use_liger_rope': True,\n",
      "           'vec_in_dim': 768},\n",
      " 'motion_score': '4',\n",
      " 'prompt': 'raining, sea',\n",
      " 'ref': '../assets/texts/i2v.png',\n",
      " 'sampling_option': {'aspect_ratio': '16:9',\n",
      "                     'guidance': 7.5,\n",
      "                     'guidance_img': 3.0,\n",
      "                     'image_osci': True,\n",
      "                     'is_causal_vae': True,\n",
      "                     'method': 'i2v',\n",
      "                     'num_frames': 129,\n",
      "                     'num_steps': 50,\n",
      "                     'resolution': '256px',\n",
      "                     'scale_temporal_osci': True,\n",
      "                     'seed': None,\n",
      "                     'shift': True,\n",
      "                     'temporal_reduction': 4,\n",
      "                     'text_osci': True},\n",
      " 'save_dir': 'samples',\n",
      " 'seed': 42,\n",
      " 't5': {'from_pretrained': 'models/google/t5-v1_1-xxl',\n",
      "        'max_length': 512,\n",
      "        'shardformer': True,\n",
      "        'type': 'text_embedder'},\n",
      " 'use_t2i2v': True}\n"
     ]
    }
   ],
   "source": [
    "logger = create_logger()\n",
    "logger.info(\"Inference configuration:\\n %s\", pformat(cfg.to_dict()))\n",
    "is_saving_process = get_is_saving_process(cfg)\n",
    "booster = get_booster(cfg)\n",
    "booster_ae = get_booster(cfg, ae=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d4ce984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-07-29 17:17:24\u001b[0m] Building dataset...\n",
      "[\u001b[34m2025-07-29 17:17:24\u001b[0m] Dataset contains 1 samples.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# 2. build dataset and dataloader\n",
    "# ======================================================\n",
    "logger.info(\"Building dataset...\")\n",
    "\n",
    "# save directory\n",
    "save_dir = cfg.save_dir\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# == build dataset ==\n",
    "if cfg.get(\"prompt\"):\n",
    "    cfg.dataset.data_path = create_tmp_csv(\n",
    "        save_dir, cfg.prompt, cfg.get(\"ref\", None), create=is_main_process()\n",
    "    )\n",
    "dist.barrier()\n",
    "dataset = build_module(cfg.dataset, DATASETS)\n",
    "\n",
    "# range selection\n",
    "start_index = cfg.get(\"start_index\", 0)\n",
    "end_index = cfg.get(\"end_index\", None)\n",
    "if end_index is None:\n",
    "    end_index = start_index + cfg.get(\"num_samples\", len(dataset.data) + 1)\n",
    "dataset.data = dataset.data[start_index:end_index]\n",
    "logger.info(\"Dataset contains %s samples.\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6936ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == build dataloader ==\n",
    "dataloader_args = dict(\n",
    "    dataset=dataset,\n",
    "    batch_size=cfg.get(\"batch_size\", 1),\n",
    "    num_workers=cfg.get(\"num_workers\", 4),\n",
    "    seed=cfg.get(\"seed\", 1024),\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    pin_memory=True,\n",
    "    process_group=get_data_parallel_group(),\n",
    "    prefetch_factor=cfg.get(\"prefetch_factor\", None),\n",
    ")\n",
    "dataloader, _ = prepare_dataloader(**dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bab76543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == prepare default params ==\n",
    "sampling_option = SamplingOption(**cfg.sampling_option)\n",
    "sampling_option = sanitize_sampling_option(sampling_option)\n",
    "\n",
    "cond_type = cfg.get(\"cond_type\", \"t2v\")\n",
    "prompt_refine = cfg.get(\"prompt_refine\", False)\n",
    "fps_save = cfg.get(\"fps_save\", 16)\n",
    "num_sample = cfg.get(\"num_sample\", 1)\n",
    "\n",
    "type_name = \"image\" if cfg.sampling_option.num_frames == 1 else \"video\"\n",
    "sub_dir = f\"{type_name}_{cfg.sampling_option.resolution}\"\n",
    "os.makedirs(os.path.join(save_dir, sub_dir), exist_ok=True)\n",
    "use_t2i2v = cfg.get(\"use_t2i2v\", False)\n",
    "img_sub_dir = os.path.join(sub_dir, \"generated_condition\")\n",
    "if use_t2i2v:\n",
    "    os.makedirs(os.path.join(save_dir, sub_dir, \"generated_condition\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410995ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 3. build model\n",
    "# ======================================================\n",
    "logger.info(\"Building models...\")\n",
    "\n",
    "# == build flux model ==\n",
    "model, model_ae, model_t5, model_clip, optional_models = prepare_models(\n",
    "    cfg, device, dtype, offload_model=cfg.get(\"offload_model\", False)\n",
    ")\n",
    "log_cuda_max_memory(\"build model\")\n",
    "\n",
    "# if booster:\n",
    "#     model, _, _, _, _ = booster.boost(model=model)\n",
    "#     model = model.unwrap()\n",
    "# if booster_ae:\n",
    "#     model_ae, _, _, _, _ = booster_ae.boost(model=model_ae)\n",
    "#     model_ae = model_ae.unwrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4defe52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_fn = prepare_api(model, model_ae, model_t5, model_clip, optional_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare image flux model if t2i2v\n",
    "if use_t2i2v:\n",
    "    api_fn_img = prepare_api(\n",
    "        optional_models[\"img_flux\"],\n",
    "        optional_models[\"img_flux_ae\"],\n",
    "        model_t5,\n",
    "        model_clip,\n",
    "        optional_models,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 4. inference\n",
    "# ======================================================\n",
    "for epoch in range(num_sample):  # generate multiple samples with different seeds\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    with tqdm(\n",
    "        enumerate(dataloader_iter, start=0),\n",
    "        desc=\"Inference progress\",\n",
    "        disable=not is_main_process(),\n",
    "        initial=0,\n",
    "        total=len(dataloader),\n",
    "    ) as pbar:\n",
    "        for _, batch in pbar:\n",
    "            original_text = batch.pop(\"text\")\n",
    "            if use_t2i2v:\n",
    "                batch[\"text\"] = (\n",
    "                    original_text\n",
    "                    if not prompt_refine\n",
    "                    else refine_prompts(original_text, type=\"t2i\")\n",
    "                )\n",
    "                sampling_option_t2i = modify_option_to_t2i(\n",
    "                    sampling_option,\n",
    "                    distilled=True,\n",
    "                    img_resolution=cfg.get(\"img_resolution\", \"768px\"),\n",
    "                )\n",
    "                if cfg.get(\"offload_model\", False):\n",
    "                    model_move_start = time.time()\n",
    "                    model = model.to(\"cpu\", dtype)\n",
    "                    model_ae = model_ae.to(\"cpu\", dtype)\n",
    "                    optional_models[\"img_flux\"].to(device, dtype)\n",
    "                    optional_models[\"img_flux_ae\"].to(device, dtype)\n",
    "                    logger.info(\n",
    "                        \"offload video diffusion model to cpu, load image flux model to gpu: %s s\",\n",
    "                        time.time() - model_move_start,\n",
    "                    )\n",
    "\n",
    "                logger.info(\"Generating image condition by flux...\")\n",
    "                x_cond = api_fn_img(\n",
    "                    sampling_option_t2i,\n",
    "                    \"t2v\",\n",
    "                    seed=sampling_option.seed + epoch if sampling_option.seed else None,\n",
    "                    channel=cfg[\"img_flux\"][\"in_channels\"],\n",
    "                    **batch,\n",
    "                ).cpu()\n",
    "\n",
    "                # save image to disk\n",
    "                batch[\"name\"] = process_and_save(\n",
    "                    x_cond,\n",
    "                    batch,\n",
    "                    cfg,\n",
    "                    img_sub_dir,\n",
    "                    sampling_option_t2i,\n",
    "                    epoch,\n",
    "                    start_index,\n",
    "                    saving=is_saving_process,\n",
    "                )\n",
    "                dist.barrier()\n",
    "\n",
    "                if cfg.get(\"offload_model\", False):\n",
    "                    model_move_start = time.time()\n",
    "                    model = model.to(device, dtype)\n",
    "                    model_ae = model_ae.to(device, dtype)\n",
    "                    optional_models[\"img_flux\"].to(\"cpu\", dtype)\n",
    "                    optional_models[\"img_flux_ae\"].to(\"cpu\", dtype)\n",
    "                    logger.info(\n",
    "                        \"load video diffusion model to gpu, offload image flux model to cpu: %s s\",\n",
    "                        time.time() - model_move_start,\n",
    "                    )\n",
    "\n",
    "                ref_dir = os.path.join(\n",
    "                    save_dir, os.path.join(sub_dir, \"generated_condition\")\n",
    "                )\n",
    "                batch[\"ref\"] = [\n",
    "                    os.path.join(ref_dir, f\"{x}.png\") for x in batch[\"name\"]\n",
    "                ]\n",
    "                cond_type = \"i2v_head\"\n",
    "\n",
    "            batch[\"text\"] = original_text\n",
    "            if prompt_refine:\n",
    "                batch[\"text\"] = refine_prompts(\n",
    "                    original_text,\n",
    "                    type=\"t2v\" if cond_type == \"t2v\" else \"t2i\",\n",
    "                    image_paths=batch.get(\"ref\", None),\n",
    "                )\n",
    "            batch[\"text\"] = add_fps_info_to_text(batch.pop(\"text\"), fps=fps_save)\n",
    "            if \"motion_score\" in cfg:\n",
    "                batch[\"text\"] = add_motion_score_to_text(\n",
    "                    batch.pop(\"text\"), cfg.get(\"motion_score\", 5)\n",
    "                )\n",
    "\n",
    "            logger.info(\"Generating video...\")\n",
    "            x = api_fn(\n",
    "                sampling_option,\n",
    "                cond_type,\n",
    "                seed=sampling_option.seed + epoch if sampling_option.seed else None,\n",
    "                patch_size=cfg.get(\"patch_size\", 2),\n",
    "                save_prefix=cfg.get(\"save_prefix\", \"\"),\n",
    "                channel=cfg[\"model\"][\"in_channels\"],\n",
    "                **batch,\n",
    "            ).cpu()\n",
    "\n",
    "            if is_saving_process:\n",
    "                process_and_save(\n",
    "                    x, batch, cfg, sub_dir, sampling_option, epoch, start_index\n",
    "                )\n",
    "            dist.barrier()\n",
    "\n",
    "logger.info(\"Inference finished.\")\n",
    "log_cuda_max_memory(\"inference\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opensora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
